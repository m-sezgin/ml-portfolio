# Teaching Machine Learning: Bridging the Gap Between Introductory Statistics and Machine Learning with Linear Regression

My name is Michele Sezgin, and I’m a computer science major at Smith College, class of 2023. In this README, I’ll be talking a little bit about myself and my portfolio. I’ve taken quite a few computer science courses at Smith, most recently taking computational machine learning, in which I’ve learned about many fundamental algorithms and concepts. In this course, we covered supervised and unsupervised learning algorithms like k-means, kNN, PCA, SVD, gradient descent, SVMs, decision trees, and NNs. Though I only have an introductory-level understanding of machine learning, I’ve coded many of the above algorithms from scratch, and have a deep understanding of how they work. I’ve also been able to apply a few of these algorithms to subjects I’m passionate about, implementing them in a book recommender system and a hockey position classification system. When I’m not coding, I’m typically playing hockey/skating, painting, embroidering, or playing guitar. 

This portfolio is the final project for CSC 294, Spring 2021 (Smith course), and is designed to bridge the gap a little between concepts covered in introductory statistics and concepts covered in machine learning. It is written with a student who has just taken introductory statistics (specifically at Smith) in mind, offering just a few ways that machine learning algorithms can aid and/or complement various statistical regression methods. For this reason, the emphasis in this portfolio is more on a conceptual analysis of machine learning algorithms (without too much technical language), with less emphasis on explaining the specifics of code (but there are still coding explanations/details where deemed necessary/relevant). This portfolio will briefly cover cross-validation, gradient descent, and principal component analysis, and discuss how each of these machine learning concepts can be useful for types of linear regression learned in introductory statistics. Each topic is divided into its own notebook, which I will sometimes call a “piece” or “component”, with its own README and Python test files. Below is a more in-depth description of each of these pieces.

Linear Regression and Cross-Validation (directory: 1-cross_val) -> This piece talks about how cross-validation can be useful for choosing the best combination of input variables to predict an output variable with linear regression. The concepts of training and testing are introduced, and a cross-validation function is coded from scratch to show how cross-validation is actually implemented.

Linear Regression and PCA (directory: 2-PCA)-> This piece introduces principal component analysis (PCA), and shows how dimension reducing our data can solve visualization issues. This piece mostly focuses on gaining an abstract and complete understanding of how PCA works, and once the foundation for understating PCA is laid, I show how we can perform linear regression on dimension reduced data.

Linear Regression and Gradient Descent (directory: 3-gradient_descent) -> This piece introduces a basic gradient descent algorithm, and shows how we can use gradient descent to find the optimal coefficients for defining a regression line. There is a strong focus on a good conceptual analysis of what gradient descent is, with some explanation of bonus machine learning concepts like runtime and epochs. Extra applications and “flavors” of gradient descent are also introduced.

After covering each of these topics, the goal is for an intro stat student to walk away from interacting with this portfolio with some extra tools and concepts under their belt that they can then use for future projects/statistical regression analyses. 
